2025-04-18 11:50:41,571 - INFO - Using GPU-build of llama.cpp
2025-04-18 11:50:41,602 - INFO - Benchmarking model SmolLM-135M.Q4_K_M.gguf ...
2025-04-18 11:50:41,605 - DEBUG - Downloading model SmolLM-135M.Q4_K_M.gguf from https://huggingface.co/QuantFactory/SmolLM-135M-GGUF/resolve/main/SmolLM-135M.Q4_K_M.gguf
2025-04-18 11:50:42,172 - DEBUG - Downloaded model SmolLM-135M.Q4_K_M.gguf (100.57 MB) in 0.57 sec (177.65 MB/s)
2025-04-18 11:50:42,174 - DEBUG - Downloading model qwen1_5-0_5b-chat-q4_k_m.gguf from https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/main/qwen1_5-0_5b-chat-q4_k_m.gguf
2025-04-18 11:50:42,175 - DEBUG - Model SmolLM-135M.Q4_K_M.gguf found at /models/SmolLM-135M.Q4_K_M.gguf (0.10 GB)
2025-04-18 11:50:43,458 - DEBUG - Downloaded model qwen1_5-0_5b-chat-q4_k_m.gguf (388.29 MB) in 1.28 sec (302.76 MB/s)
2025-04-18 11:50:43,461 - DEBUG - Downloading model gemma-2b.Q4_K_M.gguf from https://huggingface.co/mlabonne/gemma-2b-GGUF/resolve/main/gemma-2b.Q4_K_M.gguf
2025-04-18 11:50:47,762 - DEBUG - Downloaded model gemma-2b.Q4_K_M.gguf (1425.83 MB) in 4.30 sec (331.57 MB/s)
2025-04-18 11:50:47,764 - DEBUG - Downloading model llama-7b.Q4_K_M.gguf from https://huggingface.co/TheBloke/LLaMA-7b-GGUF/resolve/main/llama-7b.Q4_K_M.gguf
2025-04-18 11:50:58,598 - DEBUG - Downloaded model llama-7b.Q4_K_M.gguf (3891.95 MB) in 10.83 sec (359.26 MB/s)
2025-04-18 11:50:58,599 - DEBUG - Downloading model phi-4-q4.gguf from https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-q4.gguf
2025-04-18 11:51:22,524 - DEBUG - Using ngl 0 for model SmolLM-135M.Q4_K_M.gguf
2025-04-18 11:51:22,524 - DEBUG - Benchmarking prompt processing with 16 tokens for max 41 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7248622a69cb]
libggml-base.so(ggml_abort+0x15f)[0x7248622a6d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7248622b9c0a]
libllama.so(llama_init_from_model+0xe37)[0x7248623d0697]
./llama-bench(+0x17a63)[0x629a7a3ffa63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x724861cb1d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x724861cb1e40]
./llama-bench(+0x1bdd5)[0x629a7a403dd5]
2025-04-18 11:51:32,456 - DEBUG - Benchmarking prompt processing with 128 tokens for max 65 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x72a8a94269cb]
libggml-base.so(ggml_abort+0x15f)[0x72a8a9426d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x72a8a9439c0a]
libllama.so(llama_init_from_model+0xe37)[0x72a8a9550697]
./llama-bench(+0x17a63)[0x573876991a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x72a8a8e31d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x72a8a8e31e40]
./llama-bench(+0x1bdd5)[0x573876995dd5]
2025-04-18 11:51:42,489 - DEBUG - Benchmarking prompt processing with 512 tokens for max 104 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
2025-04-18 11:51:49,753 - DEBUG - Downloaded model phi-4-q4.gguf (8633.72 MB) in 51.15 sec (168.78 MB/s)
2025-04-18 11:51:49,755 - DEBUG - Downloading model Llama-3.3-70B-Instruct-Q4_K_M.gguf from https://huggingface.co/unsloth/Llama-3.3-70B-Instruct-GGUF/resolve/main/Llama-3.3-70B-Instruct-Q4_K_M.gguf
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7eef561ac9cb]
libggml-base.so(ggml_abort+0x15f)[0x7eef561acd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7eef561bfc0a]
libllama.so(llama_init_from_model+0xe37)[0x7eef562d6697]
./llama-bench(+0x17a63)[0x5a4ea13f6a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7eef55bb7d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7eef55bb7e40]
./llama-bench(+0x1bdd5)[0x5a4ea13fadd5]
2025-04-18 11:51:52,523 - DEBUG - Benchmarking prompt processing with 1024 tokens for max 104 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7937e89b59cb]
libggml-base.so(ggml_abort+0x15f)[0x7937e89b5d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7937e89c8c0a]
libllama.so(llama_init_from_model+0xe37)[0x7937e8adf697]
./llama-bench(+0x17a63)[0x58460d778a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7937e83c0d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7937e83c0e40]
./llama-bench(+0x1bdd5)[0x58460d77cdd5]
2025-04-18 11:52:02,505 - DEBUG - Benchmarking prompt processing with 4096 tokens for max 83 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x78038dccb9cb]
libggml-base.so(ggml_abort+0x15f)[0x78038dccbd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x78038dcdec0a]
libllama.so(llama_init_from_model+0xe37)[0x78038ddf5697]
./llama-bench(+0x17a63)[0x5f69cedfca63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x78038d6d6d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x78038d6d6e40]
./llama-bench(+0x1bdd5)[0x5f69cee00dd5]
2025-04-18 11:52:12,588 - DEBUG - Benchmarking prompt processing with 16384 tokens for max 83 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7180dc4079cb]
libggml-base.so(ggml_abort+0x15f)[0x7180dc407d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7180dc41ac0a]
libllama.so(llama_init_from_model+0xe37)[0x7180dc531697]
./llama-bench(+0x17a63)[0x62a30ed78a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7180dbe12d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7180dbe12e40]
./llama-bench(+0x1bdd5)[0x62a30ed7cdd5]
2025-04-18 11:52:22,770 - DEBUG - Benchmarking text generation with 16 tokens for max 81 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x71cda918c9cb]
libggml-base.so(ggml_abort+0x15f)[0x71cda918cd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x71cda919fc0a]
libllama.so(llama_init_from_model+0xe37)[0x71cda92b6697]
./llama-bench(+0x17a63)[0x5e812304ea63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x71cda8b97d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x71cda8b97e40]
./llama-bench(+0x1bdd5)[0x5e8123052dd5]
2025-04-18 11:52:32,652 - DEBUG - Benchmarking text generation with 128 tokens for max 129 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7544b5b239cb]
libggml-base.so(ggml_abort+0x15f)[0x7544b5b23d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7544b5b36c0a]
libllama.so(llama_init_from_model+0xe37)[0x7544b5c4d697]
./llama-bench(+0x17a63)[0x611b19861a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7544b552ed90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7544b552ee40]
./llama-bench(+0x1bdd5)[0x611b19865dd5]
2025-04-18 11:52:42,535 - DEBUG - Benchmarking text generation with 512 tokens for max 104 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7ab2866b29cb]
libggml-base.so(ggml_abort+0x15f)[0x7ab2866b2d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7ab2866c5c0a]
libllama.so(llama_init_from_model+0xe37)[0x7ab2867dc697]
./llama-bench(+0x17a63)[0x56315fdd5a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7ab2860bdd90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7ab2860bde40]
./llama-bench(+0x1bdd5)[0x56315fdd9dd5]
2025-04-18 11:52:52,717 - DEBUG - Benchmarking text generation with 1024 tokens for max 104 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x79d09fbb89cb]
libggml-base.so(ggml_abort+0x15f)[0x79d09fbb8d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x79d09fbcbc0a]
libllama.so(llama_init_from_model+0xe37)[0x79d09fce2697]
./llama-bench(+0x17a63)[0x61ed6e136a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x79d09f5c3d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x79d09f5c3e40]
./llama-bench(+0x1bdd5)[0x61ed6e13add5]
2025-04-18 11:53:02,550 - DEBUG - Benchmarking text generation with 4096 tokens for max 83 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7a7b849b99cb]
libggml-base.so(ggml_abort+0x15f)[0x7a7b849b9d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7a7b849ccc0a]
libllama.so(llama_init_from_model+0xe37)[0x7a7b84ae3697]
./llama-bench(+0x17a63)[0x5ec27785da63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7a7b843c4d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7a7b843c4e40]
./llama-bench(+0x1bdd5)[0x5ec277861dd5]
2025-04-18 11:53:12,585 - INFO - Benchmarking model qwen1_5-0_5b-chat-q4_k_m.gguf ...
2025-04-18 11:53:12,588 - DEBUG - Model qwen1_5-0_5b-chat-q4_k_m.gguf found at /models/qwen1_5-0_5b-chat-q4_k_m.gguf (0.38 GB)
2025-04-18 11:53:53,367 - DEBUG - Using ngl 0 for model qwen1_5-0_5b-chat-q4_k_m.gguf
2025-04-18 11:53:53,367 - DEBUG - Benchmarking prompt processing with 16 tokens for max 43 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7e14a0e2d9cb]
libggml-base.so(ggml_abort+0x15f)[0x7e14a0e2dd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7e14a0e40c0a]
libllama.so(llama_init_from_model+0xe37)[0x7e14a0f57697]
./llama-bench(+0x17a63)[0x625ddf6e2a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7e14a0838d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7e14a0838e40]
./llama-bench(+0x1bdd5)[0x625ddf6e6dd5]
2025-04-18 11:54:03,658 - DEBUG - Benchmarking prompt processing with 128 tokens for max 67 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
2025-04-18 11:54:10,689 - DEBUG - Downloaded model Llama-3.3-70B-Instruct-Q4_K_M.gguf (40550.61 MB) in 140.93 sec (287.73 MB/s)
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7cb59ab259cb]
libggml-base.so(ggml_abort+0x15f)[0x7cb59ab25d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7cb59ab38c0a]
libllama.so(llama_init_from_model+0xe37)[0x7cb59ac4f697]
./llama-bench(+0x17a63)[0x618eb2967a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7cb59a530d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7cb59a530e40]
./llama-bench(+0x1bdd5)[0x618eb296bdd5]
2025-04-18 11:54:13,840 - DEBUG - Benchmarking prompt processing with 512 tokens for max 105 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x76e1d4a5e9cb]
libggml-base.so(ggml_abort+0x15f)[0x76e1d4a5ed6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x76e1d4a71c0a]
libllama.so(llama_init_from_model+0xe37)[0x76e1d4b88697]
./llama-bench(+0x17a63)[0x614cd50b4a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x76e1d4469d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x76e1d4469e40]
./llama-bench(+0x1bdd5)[0x614cd50b8dd5]
2025-04-18 11:54:23,924 - DEBUG - Benchmarking prompt processing with 1024 tokens for max 105 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7d45abbd09cb]
libggml-base.so(ggml_abort+0x15f)[0x7d45abbd0d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7d45abbe3c0a]
libllama.so(llama_init_from_model+0xe37)[0x7d45abcfa697]
./llama-bench(+0x17a63)[0x5c6192c92a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7d45ab5dbd90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7d45ab5dbe40]
./llama-bench(+0x1bdd5)[0x5c6192c96dd5]
2025-04-18 11:54:34,057 - DEBUG - Benchmarking prompt processing with 4096 tokens for max 84 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7b7417b3d9cb]
libggml-base.so(ggml_abort+0x15f)[0x7b7417b3dd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7b7417b50c0a]
libllama.so(llama_init_from_model+0xe37)[0x7b7417c67697]
./llama-bench(+0x17a63)[0x639ae867ea63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7b7417548d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7b7417548e40]
./llama-bench(+0x1bdd5)[0x639ae8682dd5]
2025-04-18 11:54:44,392 - DEBUG - Benchmarking prompt processing with 16384 tokens for max 84 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7d95e12fe9cb]
libggml-base.so(ggml_abort+0x15f)[0x7d95e12fed6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7d95e1311c0a]
libllama.so(llama_init_from_model+0xe37)[0x7d95e1428697]
./llama-bench(+0x17a63)[0x5db05c38ea63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7d95e0d09d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7d95e0d09e40]
./llama-bench(+0x1bdd5)[0x5db05c392dd5]
2025-04-18 11:54:55,679 - DEBUG - Benchmarking text generation with 16 tokens for max 83 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7475ebf009cb]
libggml-base.so(ggml_abort+0x15f)[0x7475ebf00d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7475ebf13c0a]
libllama.so(llama_init_from_model+0xe37)[0x7475ec02a697]
./llama-bench(+0x17a63)[0x59405ebf7a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7475eb90bd90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7475eb90be40]
./llama-bench(+0x1bdd5)[0x59405ebfbdd5]
2025-04-18 11:55:05,764 - DEBUG - Benchmarking text generation with 128 tokens for max 131 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x77c4133bd9cb]
libggml-base.so(ggml_abort+0x15f)[0x77c4133bdd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x77c4133d0c0a]
libllama.so(llama_init_from_model+0xe37)[0x77c4134e7697]
./llama-bench(+0x17a63)[0x5d40989cea63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x77c412dc8d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x77c412dc8e40]
./llama-bench(+0x1bdd5)[0x5d40989d2dd5]
2025-04-18 11:55:15,798 - DEBUG - Benchmarking text generation with 512 tokens for max 105 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x76b45fff29cb]
libggml-base.so(ggml_abort+0x15f)[0x76b45fff2d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x76b460005c0a]
libllama.so(llama_init_from_model+0xe37)[0x76b46011c697]
./llama-bench(+0x17a63)[0x5947dada8a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x76b45f9fdd90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x76b45f9fde40]
./llama-bench(+0x1bdd5)[0x5947dadacdd5]
2025-04-18 11:55:25,882 - DEBUG - Benchmarking text generation with 1024 tokens for max 105 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x72b0422539cb]
libggml-base.so(ggml_abort+0x15f)[0x72b042253d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x72b042266c0a]
libllama.so(llama_init_from_model+0xe37)[0x72b04237d697]
./llama-bench(+0x17a63)[0x5845ef089a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x72b041c5ed90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x72b041c5ee40]
./llama-bench(+0x1bdd5)[0x5845ef08ddd5]
2025-04-18 11:55:35,964 - DEBUG - Benchmarking text generation with 4096 tokens for max 84 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x78a38ed129cb]
libggml-base.so(ggml_abort+0x15f)[0x78a38ed12d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x78a38ed25c0a]
libllama.so(llama_init_from_model+0xe37)[0x78a38ee3c697]
./llama-bench(+0x17a63)[0x60b4fe012a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x78a38e71dd90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x78a38e71de40]
./llama-bench(+0x1bdd5)[0x60b4fe016dd5]
2025-04-18 11:55:46,199 - INFO - Benchmarking model gemma-2b.Q4_K_M.gguf ...
2025-04-18 11:55:46,203 - DEBUG - Model gemma-2b.Q4_K_M.gguf found at /models/gemma-2b.Q4_K_M.gguf (1.39 GB)
2025-04-18 11:56:28,730 - DEBUG - Using ngl 0 for model gemma-2b.Q4_K_M.gguf
2025-04-18 11:56:28,730 - DEBUG - Benchmarking prompt processing with 16 tokens for max 47 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x746f3ea559cb]
libggml-base.so(ggml_abort+0x15f)[0x746f3ea55d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x746f3ea68c0a]
libllama.so(llama_init_from_model+0xe37)[0x746f3eb7f697]
./llama-bench(+0x17a63)[0x6176efe03a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x746f3e460d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x746f3e460e40]
./llama-bench(+0x1bdd5)[0x6176efe07dd5]
2025-04-18 11:56:38,963 - DEBUG - Benchmarking prompt processing with 128 tokens for max 71 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x78100c50f9cb]
libggml-base.so(ggml_abort+0x15f)[0x78100c50fd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x78100c522c0a]
libllama.so(llama_init_from_model+0xe37)[0x78100c639697]
./llama-bench(+0x17a63)[0x5f6fe4157a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x78100bf1ad90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x78100bf1ae40]
./llama-bench(+0x1bdd5)[0x5f6fe415bdd5]
2025-04-18 11:56:49,195 - DEBUG - Benchmarking prompt processing with 512 tokens for max 109 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7643604a49cb]
libggml-base.so(ggml_abort+0x15f)[0x7643604a4d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7643604b7c0a]
libllama.so(llama_init_from_model+0xe37)[0x7643605ce697]
./llama-bench(+0x17a63)[0x64681371da63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x76435feafd90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x76435feafe40]
./llama-bench(+0x1bdd5)[0x646813721dd5]
2025-04-18 11:56:59,384 - DEBUG - Benchmarking prompt processing with 1024 tokens for max 109 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7bffe55d29cb]
libggml-base.so(ggml_abort+0x15f)[0x7bffe55d2d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7bffe55e5c0a]
libllama.so(llama_init_from_model+0xe37)[0x7bffe56fc697]
./llama-bench(+0x17a63)[0x60852fde6a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7bffe4fddd90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7bffe4fdde40]
./llama-bench(+0x1bdd5)[0x60852fdeadd5]
2025-04-18 11:57:09,718 - DEBUG - Benchmarking prompt processing with 4096 tokens for max 88 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x72e1e1f4c9cb]
libggml-base.so(ggml_abort+0x15f)[0x72e1e1f4cd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x72e1e1f5fc0a]
libllama.so(llama_init_from_model+0xe37)[0x72e1e2076697]
./llama-bench(+0x17a63)[0x58f0e5f6ea63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x72e1e1957d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x72e1e1957e40]
./llama-bench(+0x1bdd5)[0x58f0e5f72dd5]
2025-04-18 11:57:20,052 - DEBUG - Benchmarking prompt processing with 16384 tokens for max 88 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7919594349cb]
libggml-base.so(ggml_abort+0x15f)[0x791959434d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x791959447c0a]
libllama.so(llama_init_from_model+0xe37)[0x79195955e697]
./llama-bench(+0x17a63)[0x58814ed77a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x791958e3fd90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x791958e3fe40]
./llama-bench(+0x1bdd5)[0x58814ed7bdd5]
2025-04-18 11:57:30,485 - DEBUG - Benchmarking text generation with 16 tokens for max 87 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x79005022a9cb]
libggml-base.so(ggml_abort+0x15f)[0x79005022ad6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x79005023dc0a]
libllama.so(llama_init_from_model+0xe37)[0x790050354697]
./llama-bench(+0x17a63)[0x5d2430310a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x79004fc35d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x79004fc35e40]
./llama-bench(+0x1bdd5)[0x5d2430314dd5]
2025-04-18 11:57:40,869 - DEBUG - Benchmarking text generation with 128 tokens for max 135 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7356f9eb69cb]
libggml-base.so(ggml_abort+0x15f)[0x7356f9eb6d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7356f9ec9c0a]
libllama.so(llama_init_from_model+0xe37)[0x7356f9fe0697]
./llama-bench(+0x17a63)[0x589acf549a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7356f98c1d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7356f98c1e40]
./llama-bench(+0x1bdd5)[0x589acf54ddd5]
2025-04-18 11:57:51,102 - DEBUG - Benchmarking text generation with 512 tokens for max 109 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x70673cdec9cb]
libggml-base.so(ggml_abort+0x15f)[0x70673cdecd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x70673cdffc0a]
libllama.so(llama_init_from_model+0xe37)[0x70673cf16697]
./llama-bench(+0x17a63)[0x6326518b9a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x70673c7f7d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x70673c7f7e40]
./llama-bench(+0x1bdd5)[0x6326518bddd5]
2025-04-18 11:58:01,384 - DEBUG - Benchmarking text generation with 1024 tokens for max 109 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7259e3f629cb]
libggml-base.so(ggml_abort+0x15f)[0x7259e3f62d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7259e3f75c0a]
libllama.so(llama_init_from_model+0xe37)[0x7259e408c697]
./llama-bench(+0x17a63)[0x6288706c2a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7259e396dd90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7259e396de40]
./llama-bench(+0x1bdd5)[0x6288706c6dd5]
2025-04-18 11:58:11,715 - DEBUG - Benchmarking text generation with 4096 tokens for max 88 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x70fb1039b9cb]
libggml-base.so(ggml_abort+0x15f)[0x70fb1039bd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x70fb103aec0a]
libllama.so(llama_init_from_model+0xe37)[0x70fb104c5697]
./llama-bench(+0x17a63)[0x651bae138a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x70fb0fda6d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x70fb0fda6e40]
./llama-bench(+0x1bdd5)[0x651bae13cdd5]
2025-04-18 11:58:21,998 - INFO - Benchmarking model llama-7b.Q4_K_M.gguf ...
2025-04-18 11:58:22,002 - DEBUG - Model llama-7b.Q4_K_M.gguf found at /models/llama-7b.Q4_K_M.gguf (3.80 GB)
2025-04-18 11:59:05,388 - DEBUG - Using ngl 0 for model llama-7b.Q4_K_M.gguf
2025-04-18 11:59:05,388 - DEBUG - Benchmarking prompt processing with 16 tokens for max 56 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x72ad254689cb]
libggml-base.so(ggml_abort+0x15f)[0x72ad25468d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x72ad2547bc0a]
libllama.so(llama_init_from_model+0xe37)[0x72ad25592697]
./llama-bench(+0x17a63)[0x5af364c04a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x72ad24e73d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x72ad24e73e40]
./llama-bench(+0x1bdd5)[0x5af364c08dd5]
2025-04-18 11:59:15,571 - DEBUG - Benchmarking prompt processing with 128 tokens for max 80 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x71d964dd69cb]
libggml-base.so(ggml_abort+0x15f)[0x71d964dd6d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x71d964de9c0a]
libllama.so(llama_init_from_model+0xe37)[0x71d964f00697]
./llama-bench(+0x17a63)[0x63264a53aa63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x71d9647e1d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x71d9647e1e40]
./llama-bench(+0x1bdd5)[0x63264a53edd5]
2025-04-18 11:59:25,854 - DEBUG - Benchmarking prompt processing with 512 tokens for max 119 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x78bdc44299cb]
libggml-base.so(ggml_abort+0x15f)[0x78bdc4429d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x78bdc443cc0a]
libllama.so(llama_init_from_model+0xe37)[0x78bdc4553697]
./llama-bench(+0x17a63)[0x6364db0cba63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x78bdc3e34d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x78bdc3e34e40]
./llama-bench(+0x1bdd5)[0x6364db0cfdd5]
2025-04-18 11:59:36,038 - DEBUG - Benchmarking prompt processing with 1024 tokens for max 119 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7d32faaf49cb]
libggml-base.so(ggml_abort+0x15f)[0x7d32faaf4d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7d32fab07c0a]
libllama.so(llama_init_from_model+0xe37)[0x7d32fac1e697]
./llama-bench(+0x17a63)[0x56ad85ca8a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7d32fa4ffd90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7d32fa4ffe40]
./llama-bench(+0x1bdd5)[0x56ad85cacdd5]
2025-04-18 11:59:46,421 - DEBUG - Benchmarking prompt processing with 4096 tokens for max 98 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7d604d51d9cb]
libggml-base.so(ggml_abort+0x15f)[0x7d604d51dd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7d604d530c0a]
libllama.so(llama_init_from_model+0xe37)[0x7d604d647697]
./llama-bench(+0x17a63)[0x635dde200a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7d604cf28d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7d604cf28e40]
./llama-bench(+0x1bdd5)[0x635dde204dd5]
2025-04-18 11:59:57,956 - DEBUG - Benchmarking prompt processing with 16384 tokens for max 98 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x746c757cb9cb]
libggml-base.so(ggml_abort+0x15f)[0x746c757cbd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x746c757dec0a]
libllama.so(llama_init_from_model+0xe37)[0x746c758f5697]
./llama-bench(+0x17a63)[0x60fb87773a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x746c751d6d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x746c751d6e40]
./llama-bench(+0x1bdd5)[0x60fb87777dd5]
2025-04-18 12:00:13,598 - DEBUG - Benchmarking text generation with 16 tokens for max 96 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x79cc004769cb]
libggml-base.so(ggml_abort+0x15f)[0x79cc00476d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x79cc00489c0a]
libllama.so(llama_init_from_model+0xe37)[0x79cc005a0697]
./llama-bench(+0x17a63)[0x617debeafa63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x79cbffe81d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x79cbffe81e40]
./llama-bench(+0x1bdd5)[0x617debeb3dd5]
2025-04-18 12:00:23,881 - DEBUG - Benchmarking text generation with 128 tokens for max 144 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x73bea63fa9cb]
libggml-base.so(ggml_abort+0x15f)[0x73bea63fad6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x73bea640dc0a]
libllama.so(llama_init_from_model+0xe37)[0x73bea6524697]
./llama-bench(+0x17a63)[0x5e6eab6eba63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x73bea5e05d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x73bea5e05e40]
./llama-bench(+0x1bdd5)[0x5e6eab6efdd5]
2025-04-18 12:00:34,013 - DEBUG - Benchmarking text generation with 512 tokens for max 119 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x794d3d1c19cb]
libggml-base.so(ggml_abort+0x15f)[0x794d3d1c1d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x794d3d1d4c0a]
libllama.so(llama_init_from_model+0xe37)[0x794d3d2eb697]
./llama-bench(+0x17a63)[0x651789505a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x794d3cbccd90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x794d3cbcce40]
./llama-bench(+0x1bdd5)[0x651789509dd5]
2025-04-18 12:00:44,194 - DEBUG - Benchmarking text generation with 1024 tokens for max 119 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x76cfb1e7e9cb]
libggml-base.so(ggml_abort+0x15f)[0x76cfb1e7ed6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x76cfb1e91c0a]
libllama.so(llama_init_from_model+0xe37)[0x76cfb1fa8697]
./llama-bench(+0x17a63)[0x56cbb93f6a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x76cfb1889d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x76cfb1889e40]
./llama-bench(+0x1bdd5)[0x56cbb93fadd5]
2025-04-18 12:00:54,525 - DEBUG - Benchmarking text generation with 4096 tokens for max 98 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7bc293c899cb]
libggml-base.so(ggml_abort+0x15f)[0x7bc293c89d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7bc293c9cc0a]
libllama.so(llama_init_from_model+0xe37)[0x7bc293db3697]
./llama-bench(+0x17a63)[0x5cd47318aa63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7bc293694d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7bc293694e40]
./llama-bench(+0x1bdd5)[0x5cd47318edd5]
2025-04-18 12:01:06,112 - INFO - Benchmarking model phi-4-q4.gguf ...
2025-04-18 12:01:06,115 - DEBUG - Model phi-4-q4.gguf found at /models/phi-4-q4.gguf (8.43 GB)
2025-04-18 12:01:54,068 - DEBUG - Using ngl 0 for model phi-4-q4.gguf
2025-04-18 12:01:54,069 - DEBUG - Benchmarking prompt processing with 16 tokens for max 75 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7583e661e9cb]
libggml-base.so(ggml_abort+0x15f)[0x7583e661ed6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7583e6631c0a]
libllama.so(llama_init_from_model+0xe37)[0x7583e6748697]
./llama-bench(+0x17a63)[0x5ca7d61a9a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7583e6029d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7583e6029e40]
./llama-bench(+0x1bdd5)[0x5ca7d61addd5]
2025-04-18 12:02:04,603 - DEBUG - Benchmarking prompt processing with 128 tokens for max 99 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7bb76d0cb9cb]
libggml-base.so(ggml_abort+0x15f)[0x7bb76d0cbd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7bb76d0dec0a]
libllama.so(llama_init_from_model+0xe37)[0x7bb76d1f5697]
./llama-bench(+0x17a63)[0x65233609ca63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7bb76cad6d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7bb76cad6e40]
./llama-bench(+0x1bdd5)[0x6523360a0dd5]
2025-04-18 12:02:15,287 - DEBUG - Benchmarking prompt processing with 512 tokens for max 137 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x77a973e699cb]
libggml-base.so(ggml_abort+0x15f)[0x77a973e69d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x77a973e7cc0a]
libllama.so(llama_init_from_model+0xe37)[0x77a973f93697]
./llama-bench(+0x17a63)[0x5c31c99d1a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x77a973874d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x77a973874e40]
./llama-bench(+0x1bdd5)[0x5c31c99d5dd5]
2025-04-18 12:02:25,769 - DEBUG - Benchmarking prompt processing with 1024 tokens for max 137 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x75989b38d9cb]
libggml-base.so(ggml_abort+0x15f)[0x75989b38dd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x75989b3a0c0a]
libllama.so(llama_init_from_model+0xe37)[0x75989b4b7697]
./llama-bench(+0x17a63)[0x62d904947a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x75989ad98d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x75989ad98e40]
./llama-bench(+0x1bdd5)[0x62d90494bdd5]
2025-04-18 12:02:36,502 - DEBUG - Benchmarking prompt processing with 4096 tokens for max 117 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x77b93f05a9cb]
libggml-base.so(ggml_abort+0x15f)[0x77b93f05ad6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x77b93f06dc0a]
libllama.so(llama_init_from_model+0xe37)[0x77b93f184697]
./llama-bench(+0x17a63)[0x62fdccc54a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x77b93ea65d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x77b93ea65e40]
./llama-bench(+0x1bdd5)[0x62fdccc58dd5]
2025-04-18 12:02:47,636 - DEBUG - Benchmarking prompt processing with 16384 tokens for max 117 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7cb1139b19cb]
libggml-base.so(ggml_abort+0x15f)[0x7cb1139b1d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7cb1139c4c0a]
libllama.so(llama_init_from_model+0xe37)[0x7cb113adb697]
./llama-bench(+0x17a63)[0x5c70bb55ba63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7cb1133bcd90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7cb1133bce40]
./llama-bench(+0x1bdd5)[0x5c70bb55fdd5]
2025-04-18 12:03:00,523 - DEBUG - Benchmarking text generation with 16 tokens for max 115 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7471bf27c9cb]
libggml-base.so(ggml_abort+0x15f)[0x7471bf27cd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7471bf28fc0a]
libllama.so(llama_init_from_model+0xe37)[0x7471bf3a6697]
./llama-bench(+0x17a63)[0x5bf84ff2ca63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7471bec87d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7471bec87e40]
./llama-bench(+0x1bdd5)[0x5bf84ff30dd5]
2025-04-18 12:03:11,205 - DEBUG - Benchmarking text generation with 128 tokens for max 163 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x737a19b059cb]
libggml-base.so(ggml_abort+0x15f)[0x737a19b05d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x737a19b18c0a]
libllama.so(llama_init_from_model+0xe37)[0x737a19c2f697]
./llama-bench(+0x17a63)[0x573e51f70a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x737a19510d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x737a19510e40]
./llama-bench(+0x1bdd5)[0x573e51f74dd5]
2025-04-18 12:03:21,737 - DEBUG - Benchmarking text generation with 512 tokens for max 137 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7e2f34b249cb]
libggml-base.so(ggml_abort+0x15f)[0x7e2f34b24d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7e2f34b37c0a]
libllama.so(llama_init_from_model+0xe37)[0x7e2f34c4e697]
./llama-bench(+0x17a63)[0x5d268ff63a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7e2f3452fd90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7e2f3452fe40]
./llama-bench(+0x1bdd5)[0x5d268ff67dd5]
2025-04-18 12:03:32,470 - DEBUG - Benchmarking text generation with 1024 tokens for max 137 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7a186c68b9cb]
libggml-base.so(ggml_abort+0x15f)[0x7a186c68bd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7a186c69ec0a]
libllama.so(llama_init_from_model+0xe37)[0x7a186c7b5697]
./llama-bench(+0x17a63)[0x63fde50e7a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7a186c096d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7a186c096e40]
./llama-bench(+0x1bdd5)[0x63fde50ebdd5]
2025-04-18 12:03:43,155 - DEBUG - Benchmarking text generation with 4096 tokens for max 117 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7f3872ab59cb]
libggml-base.so(ggml_abort+0x15f)[0x7f3872ab5d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7f3872ac8c0a]
libllama.so(llama_init_from_model+0xe37)[0x7f3872bdf697]
./llama-bench(+0x17a63)[0x5b882836fa63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7f38724c0d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7f38724c0e40]
./llama-bench(+0x1bdd5)[0x5b8828373dd5]
2025-04-18 12:03:54,239 - INFO - Benchmarking model Llama-3.3-70B-Instruct-Q4_K_M.gguf ...
2025-04-18 12:03:54,242 - DEBUG - Model Llama-3.3-70B-Instruct-Q4_K_M.gguf found at /models/Llama-3.3-70B-Instruct-Q4_K_M.gguf (39.60 GB)
2025-04-18 12:05:05,623 - DEBUG - Using ngl 0 for model Llama-3.3-70B-Instruct-Q4_K_M.gguf
2025-04-18 12:05:05,623 - DEBUG - Benchmarking prompt processing with 16 tokens for max 199 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7acf441f79cb]
libggml-base.so(ggml_abort+0x15f)[0x7acf441f7d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7acf4420ac0a]
libllama.so(llama_init_from_model+0xe37)[0x7acf44321697]
./llama-bench(+0x17a63)[0x5fd3382f4a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7acf43c02d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7acf43c02e40]
./llama-bench(+0x1bdd5)[0x5fd3382f8dd5]
2025-04-18 12:05:18,661 - DEBUG - Benchmarking prompt processing with 128 tokens for max 223 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7247930789cb]
libggml-base.so(ggml_abort+0x15f)[0x724793078d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x72479308bc0a]
libllama.so(llama_init_from_model+0xe37)[0x7247931a2697]
./llama-bench(+0x17a63)[0x5db3fad67a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x724792a83d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x724792a83e40]
./llama-bench(+0x1bdd5)[0x5db3fad6bdd5]
2025-04-18 12:05:31,746 - DEBUG - Benchmarking prompt processing with 512 tokens for max 262 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7db1c814c9cb]
libggml-base.so(ggml_abort+0x15f)[0x7db1c814cd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7db1c815fc0a]
libllama.so(llama_init_from_model+0xe37)[0x7db1c8276697]
./llama-bench(+0x17a63)[0x5bc0bd0b3a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7db1c7b57d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7db1c7b57e40]
./llama-bench(+0x1bdd5)[0x5bc0bd0b7dd5]
2025-04-18 12:05:44,883 - DEBUG - Benchmarking prompt processing with 1024 tokens for max 262 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x757d0a54c9cb]
libggml-base.so(ggml_abort+0x15f)[0x757d0a54cd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x757d0a55fc0a]
libllama.so(llama_init_from_model+0xe37)[0x757d0a676697]
./llama-bench(+0x17a63)[0x58247ee58a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x757d09f57d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x757d09f57e40]
./llama-bench(+0x1bdd5)[0x58247ee5cdd5]
2025-04-18 12:05:58,068 - DEBUG - Benchmarking prompt processing with 4096 tokens for max 241 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7256a60cc9cb]
libggml-base.so(ggml_abort+0x15f)[0x7256a60ccd6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7256a60dfc0a]
libllama.so(llama_init_from_model+0xe37)[0x7256a61f6697]
./llama-bench(+0x17a63)[0x5fc2e331aa63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7256a5ad7d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7256a5ad7e40]
./llama-bench(+0x1bdd5)[0x5fc2e331edd5]
2025-04-18 12:06:11,808 - DEBUG - Benchmarking prompt processing with 16384 tokens for max 241 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7ec01cae29cb]
libggml-base.so(ggml_abort+0x15f)[0x7ec01cae2d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7ec01caf5c0a]
libllama.so(llama_init_from_model+0xe37)[0x7ec01cc0c697]
./llama-bench(+0x17a63)[0x604c26e4da63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7ec01c4edd90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7ec01c4ede40]
./llama-bench(+0x1bdd5)[0x604c26e51dd5]
2025-04-18 12:06:28,253 - DEBUG - Benchmarking text generation with 16 tokens for max 239 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7182919609cb]
libggml-base.so(ggml_abort+0x15f)[0x718291960d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x718291973c0a]
libllama.so(llama_init_from_model+0xe37)[0x718291a8a697]
./llama-bench(+0x17a63)[0x62d682ff6a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x71829136bd90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x71829136be40]
./llama-bench(+0x1bdd5)[0x62d682ffadd5]
2025-04-18 12:06:41,091 - DEBUG - Benchmarking text generation with 128 tokens for max 287 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x73c0e243e9cb]
libggml-base.so(ggml_abort+0x15f)[0x73c0e243ed6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x73c0e2451c0a]
libllama.so(llama_init_from_model+0xe37)[0x73c0e2568697]
./llama-bench(+0x17a63)[0x593f5b0dea63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x73c0e1e49d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x73c0e1e49e40]
./llama-bench(+0x1bdd5)[0x593f5b0e2dd5]
2025-04-18 12:06:53,979 - DEBUG - Benchmarking text generation with 512 tokens for max 262 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x795e0a5819cb]
libggml-base.so(ggml_abort+0x15f)[0x795e0a581d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x795e0a594c0a]
libllama.so(llama_init_from_model+0xe37)[0x795e0a6ab697]
./llama-bench(+0x17a63)[0x5783d9f9fa63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x795e09f8cd90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x795e09f8ce40]
./llama-bench(+0x1bdd5)[0x5783d9fa3dd5]
2025-04-18 12:07:06,917 - DEBUG - Benchmarking text generation with 1024 tokens for max 262 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x7ad6cd8c89cb]
libggml-base.so(ggml_abort+0x15f)[0x7ad6cd8c8d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x7ad6cd8dbc0a]
libllama.so(llama_init_from_model+0xe37)[0x7ad6cd9f2697]
./llama-bench(+0x17a63)[0x583fefa9ba63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7ad6cd2d3d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7ad6cd2d3e40]
./llama-bench(+0x1bdd5)[0x583fefa9fdd5]
2025-04-18 12:07:20,054 - DEBUG - Benchmarking text generation with 4096 tokens for max 241 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 16 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 8: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 9: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 10: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 11: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 12: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 13: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 14: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
  Device 15: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
/app/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
libggml-base.so(+0x159cb)[0x736dad0999cb]
libggml-base.so(ggml_abort+0x15f)[0x736dad099d6f]
libggml-base.so(ggml_backend_sched_new+0x31a)[0x736dad0acc0a]
libllama.so(llama_init_from_model+0xe37)[0x736dad1c3697]
./llama-bench(+0x17a63)[0x5efdab2c8a63]
/usr/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x736dacaa4d90]
/usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x736dacaa4e40]
./llama-bench(+0x1bdd5)[0x5efdab2ccdd5]

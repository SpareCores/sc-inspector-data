2025-12-21 04:11:12,091 - INFO - Using GPU-build of llama.cpp
2025-12-21 04:11:12,182 - INFO - Benchmarking model SmolLM-135M.Q4_K_M.gguf ...
2025-12-21 04:11:12,433 - DEBUG - Downloading model SmolLM-135M.Q4_K_M.gguf from https://huggingface.co/QuantFactory/SmolLM-135M-GGUF/resolve/b35f229108e4c5a4deeccccf9bf75cd46006fa7b/SmolLM-135M.Q4_K_M.gguf
2025-12-21 04:11:13,228 - DEBUG - Downloaded model SmolLM-135M.Q4_K_M.gguf (100.57 MB) in 0.79 sec (127.01 MB/s)
2025-12-21 04:11:13,233 - DEBUG - Model SmolLM-135M.Q4_K_M.gguf found at /models/SmolLM-135M.Q4_K_M.gguf (0.10 GB)
2025-12-21 04:11:13,482 - DEBUG - Downloading model qwen1_5-0_5b-chat-q4_k_m.gguf from https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/cfab082d2fef4a8736ef384dc764c2fb6887f387/qwen1_5-0_5b-chat-q4_k_m.gguf
2025-12-21 04:11:15,673 - DEBUG - Downloaded model qwen1_5-0_5b-chat-q4_k_m.gguf (388.29 MB) in 2.19 sec (177.32 MB/s)
2025-12-21 04:11:15,867 - DEBUG - Downloading model gemma-2b.Q4_K_M.gguf from https://huggingface.co/mlabonne/gemma-2b-GGUF/resolve/8dd617a80664da2853999b6b1206595f1c33daa2/gemma-2b.Q4_K_M.gguf
2025-12-21 04:11:16,272 - DEBUG - Using ngl 999 for model SmolLM-135M.Q4_K_M.gguf
2025-12-21 04:11:16,272 - DEBUG - Benchmarking prompt processing with 16 tokens for max 41 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:11:17,744 - DEBUG - Benchmarking prompt processing with 128 tokens for max 65 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:11:19,267 - DEBUG - Benchmarking prompt processing with 512 tokens for max 104 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:11:20,842 - DEBUG - Benchmarking prompt processing with 1024 tokens for max 104 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:11:22,465 - DEBUG - Benchmarking prompt processing with 4096 tokens for max 83 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:11:23,545 - DEBUG - Downloaded model gemma-2b.Q4_K_M.gguf (1425.83 MB) in 7.68 sec (185.73 MB/s)
2025-12-21 04:11:23,743 - DEBUG - Downloading model llama-7b.Q4_K_M.gguf from https://huggingface.co/TheBloke/LLaMA-7b-GGUF/resolve/0a1200190c8c1baafaa0b496c599ba68ad3d4056/llama-7b.Q4_K_M.gguf
2025-12-21 04:11:24,693 - DEBUG - Benchmarking prompt processing with 16384 tokens for max 83 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:11:31,748 - DEBUG - Benchmarking text generation with 16 tokens for max 81 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:11:33,372 - DEBUG - Benchmarking text generation with 128 tokens for max 129 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:11:37,107 - DEBUG - Benchmarking text generation with 512 tokens for max 104 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:11:48,009 - DEBUG - Downloaded model llama-7b.Q4_K_M.gguf (3891.95 MB) in 24.26 sec (160.39 MB/s)
2025-12-21 04:11:48,203 - DEBUG - Downloading model phi-4-q4.gguf from https://huggingface.co/microsoft/phi-4-gguf/resolve/b1e764cfdbdd0a3ed824d6a8424129eb0a2232ff/phi-4-q4.gguf
2025-12-21 04:11:48,731 - DEBUG - Benchmarking text generation with 1024 tokens for max 104 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:12:12,439 - ERROR - Skipping text generation benchmarks with 1024+ tokens as it's unlikely to hit the expected 250 tokens/sec.
2025-12-21 04:12:12,439 - INFO - Benchmarking model qwen1_5-0_5b-chat-q4_k_m.gguf ...
2025-12-21 04:12:12,447 - DEBUG - Model qwen1_5-0_5b-chat-q4_k_m.gguf found at /models/qwen1_5-0_5b-chat-q4_k_m.gguf (0.38 GB)
2025-12-21 04:12:15,453 - DEBUG - Using ngl 999 for model qwen1_5-0_5b-chat-q4_k_m.gguf
2025-12-21 04:12:15,453 - DEBUG - Benchmarking prompt processing with 16 tokens for max 43 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:12:17,229 - DEBUG - Benchmarking prompt processing with 128 tokens for max 67 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:12:19,204 - DEBUG - Benchmarking prompt processing with 512 tokens for max 105 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:12:21,279 - DEBUG - Benchmarking prompt processing with 1024 tokens for max 105 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:12:23,354 - DEBUG - Benchmarking prompt processing with 4096 tokens for max 84 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:12:26,485 - DEBUG - Benchmarking prompt processing with 16384 tokens for max 84 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:12:35,848 - DEBUG - Benchmarking text generation with 16 tokens for max 83 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:12:37,673 - DEBUG - Benchmarking text generation with 128 tokens for max 131 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:12:41,908 - DEBUG - Benchmarking text generation with 512 tokens for max 105 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:12:46,958 - DEBUG - Downloaded model phi-4-q4.gguf (8633.72 MB) in 58.75 sec (146.95 MB/s)
2025-12-21 04:12:47,287 - INFO - Not enough free disk space for model Llama-3.3-70B-Instruct-Q4_K_M.gguf (14975.21 MB free, 44605.67 MB needed), skipping download
2025-12-21 04:12:53,634 - DEBUG - Benchmarking text generation with 1024 tokens for max 105 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:13:16,166 - DEBUG - Benchmarking text generation with 4096 tokens for max 84 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:14:40,354 - ERROR - Error: Command '['./llama-bench', '-t', '36', '-sm', 'layer', '-fa', '1', '-ub', '512', '-b', '2048', '-o', 'jsonl', '-m', '/models/qwen1_5-0_5b-chat-q4_k_m.gguf', '-ngl', '999', '-n', '4096', '-p', '0']' timed out after 83.999961665 seconds
2025-12-21 04:14:40,354 - INFO - Benchmarking model gemma-2b.Q4_K_M.gguf ...
2025-12-21 04:14:40,361 - DEBUG - Model gemma-2b.Q4_K_M.gguf found at /models/gemma-2b.Q4_K_M.gguf (1.39 GB)
2025-12-21 04:14:44,403 - DEBUG - Using ngl 999 for model gemma-2b.Q4_K_M.gguf
2025-12-21 04:14:44,403 - DEBUG - Benchmarking prompt processing with 16 tokens for max 47 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:14:46,729 - DEBUG - Benchmarking prompt processing with 128 tokens for max 71 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:14:49,256 - DEBUG - Benchmarking prompt processing with 512 tokens for max 109 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:14:51,733 - DEBUG - Benchmarking prompt processing with 1024 tokens for max 109 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:14:54,964 - DEBUG - Benchmarking prompt processing with 4096 tokens for max 88 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:15:01,161 - DEBUG - Benchmarking prompt processing with 16384 tokens for max 88 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:15:29,457 - DEBUG - Benchmarking text generation with 16 tokens for max 87 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:15:32,085 - DEBUG - Benchmarking text generation with 128 tokens for max 135 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:15:37,675 - DEBUG - Benchmarking text generation with 512 tokens for max 109 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:15:53,762 - DEBUG - Benchmarking text generation with 1024 tokens for max 109 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:16:24,566 - ERROR - Skipping text generation benchmarks with 1024+ tokens as it's unlikely to hit the expected 250 tokens/sec.
2025-12-21 04:16:24,566 - INFO - Benchmarking model llama-7b.Q4_K_M.gguf ...
2025-12-21 04:16:24,574 - DEBUG - Model llama-7b.Q4_K_M.gguf found at /models/llama-7b.Q4_K_M.gguf (3.80 GB)
2025-12-21 04:16:30,257 - DEBUG - Using ngl 999 for model llama-7b.Q4_K_M.gguf
2025-12-21 04:16:30,257 - DEBUG - Benchmarking prompt processing with 16 tokens for max 56 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:16:32,532 - DEBUG - Benchmarking prompt processing with 128 tokens for max 80 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:16:35,713 - DEBUG - Benchmarking prompt processing with 512 tokens for max 119 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:16:39,446 - DEBUG - Benchmarking prompt processing with 1024 tokens for max 119 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:16:43,881 - DEBUG - Benchmarking prompt processing with 4096 tokens for max 98 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:16:56,149 - DEBUG - Benchmarking prompt processing with 16384 tokens for max 98 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:18:12,401 - DEBUG - Benchmarking text generation with 16 tokens for max 96 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:18:15,733 - DEBUG - Benchmarking text generation with 128 tokens for max 144 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:18:23,885 - DEBUG - Benchmarking text generation with 512 tokens for max 119 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:18:49,217 - DEBUG - Benchmarking text generation with 1024 tokens for max 119 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:19:39,048 - ERROR - Skipping text generation benchmarks with 1024+ tokens as it's unlikely to hit the expected 250 tokens/sec.
2025-12-21 04:19:39,048 - INFO - Benchmarking model phi-4-q4.gguf ...
2025-12-21 04:19:39,077 - DEBUG - Model phi-4-q4.gguf found at /models/phi-4-q4.gguf (8.43 GB)
2025-12-21 04:19:48,327 - DEBUG - Using ngl 999 for model phi-4-q4.gguf
2025-12-21 04:19:48,327 - DEBUG - Benchmarking prompt processing with 16 tokens for max 75 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:19:52,461 - DEBUG - Benchmarking prompt processing with 128 tokens for max 99 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:19:57,840 - DEBUG - Benchmarking prompt processing with 512 tokens for max 137 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:20:03,880 - DEBUG - Benchmarking prompt processing with 1024 tokens for max 137 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:20:12,084 - DEBUG - Benchmarking prompt processing with 4096 tokens for max 117 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:20:33,944 - DEBUG - Benchmarking prompt processing with 16384 tokens for max 117 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:22:31,165 - ERROR - Error: Command '['./llama-bench', '-t', '36', '-sm', 'layer', '-fa', '1', '-ub', '512', '-b', '2048', '-o', 'jsonl', '-m', '/models/phi-4-q4.gguf', '-ngl', '999', '-p', '16384', '-n', '0']' timed out after 116.99996524800008 seconds
2025-12-21 04:22:31,166 - DEBUG - Benchmarking text generation with 16 tokens for max 115 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:22:36,002 - DEBUG - Benchmarking text generation with 128 tokens for max 163 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:22:49,727 - DEBUG - Benchmarking text generation with 512 tokens for max 137 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:23:32,736 - DEBUG - Benchmarking text generation with 1024 tokens for max 137 sec
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 4 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 1: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 2: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  Device 3: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
2025-12-21 04:24:56,522 - ERROR - Skipping text generation benchmarks with 1024+ tokens as it's unlikely to hit the expected 250 tokens/sec.
2025-12-21 04:24:56,523 - INFO - Benchmarking model Llama-3.3-70B-Instruct-Q4_K_M.gguf ...
2025-12-21 04:24:56,534 - ERROR - Model Llama-3.3-70B-Instruct-Q4_K_M.gguf was not downloaded, skipping benchmark.
